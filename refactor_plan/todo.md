# French Sustainability Transcript Analysis - Refactor TODO

## Project Overview
Transform the existing Gemini LangGraph research assistant into a French sustainability transcript analysis tool that identifies and categorizes paradoxes/tensions in organizational sustainability discussions.

## Key Requirements
- **Input**: French transcripts (~300 pages) from group discussions (Groups A-H)
- **Output**: Structured analysis with 12 fields per identified tension/paradox
- **Target Fields**:
  - Concepts de 2nd ordre (Second-order concepts)
  - Items de 1er ordre reformulÃ© (Reformulated first-order items)
  - Items de 1er ordre (intitulÃ© d'origine) (Original first-order items)
  - DÃ©tails (Details - text excerpts)
  - SynthÃ¨se (Synthesis)
  - Code Entretien (Interview code)
  - PÃ©riode (Period: 2023/2050)
  - ThÃ¨me (Theme: LÃ©gitimitÃ©/Performance)
  - Code spÃ© (Specific code)
  - Constat/StÃ©rÃ©otype + IFa/IFr (Observation/Stereotype + Facilitating/Hindering)
  - Tension de modÃ¨le (Model tension)
  - Tension liÃ©e au changement (Change-related tension)

## Phase 1: Backend Refactoring âœ… COMPLETED & SCALE VALIDATED

### 1.1 Core Graph Architecture (`backend/src/agent/graph.py`) âœ…
- [x] **Replace web research nodes** with transcript analysis nodes:
  - [x] `segment_transcript` - Extract tension-containing segments
  - [x] `analyze_segment` - Analyze individual segments (parallel processing)
  - [x] `finalize_output` - Aggregate and format results
- [x] **Update graph flow**:
  - [x] START â†’ `segment_transcript`
  - [x] `segment_transcript` â†’ spawn multiple `analyze_segment` (parallel)
  - [x] `analyze_segment` â†’ `finalize_output`
  - [x] `finalize_output` â†’ END
- [x] **Remove old nodes**: `generate_query`, `web_research`, `reflection`, `finalize_answer`
- [x] **Integration with data pipeline**: Successfully processes `Table_A_ml_ready.json`

### 1.2 State Management (`backend/src/agent/state.py`) âœ…
- [x] **Define new state schemas**:
  - [x] `OverallState` - Complete workflow state with transcript and results
  - [x] `SegmentationState` - List of extracted segments
  - [x] `AnalysisResult` - Single tension analysis result
- [x] **Remove old states**: `QueryGenerationState`, `WebSearchState`, `ReflectionState`
- [x] **Pipeline integration**: Handles preprocessed data from data engineering pipeline

### 1.3 Prompt Engineering (`backend/src/agent/prompts.py`) âœ…
- [x] **Create French-language prompts**:
  - [x] `segmentation_instructions` - Find paradox excerpts using contrastive markers
  - [x] `tension_extraction_instructions` - Extract original/reformulated items
  - [x] `categorization_instructions` - Assign second-order concepts and codes
  - [x] `synthesis_instructions` - Generate one-line summaries
  - [x] `imaginaire_classification_instructions` - C/S and IFa/IFr classification
- [x] **Include few-shot examples** in French
- [x] **Remove old prompts**: search-related templates
- [x] **Tested with real data**: Successfully analyzes French sustainability transcripts

### 1.4 Schemas and Tools (`backend/src/agent/tools_and_schemas.py`) âœ…
- [x] **Define Pydantic models**:
  - [x] `SegmentsList` - Segmentation output
  - [x] `TensionExtraction` - Original/reformulated items
  - [x] `Categorization` - Concept, code, theme classification
  - [x] `FullAnalysisResult` - Complete analysis for one segment
- [x] **Create domain tools**:
  - [x] Taxonomy mapping dictionary (concept â†’ code)
  - [x] Theme keywords for classification
- [x] **Remove old schemas**: `SearchQueryList`, `Reflection`
- [x] **Domain expertise**: Successfully maps tensions to sustainability concepts

### 1.5 Utilities (`backend/src/agent/utils.py`) âœ…
- [x] **Add new utilities**:
  - [x] `clean_transcript()` - Remove timestamps/artifacts
  - [x] `detect_period()` - Extract temporal markers (2023/2050)
  - [x] `determine_theme()` - Keyword-based theme classification (LÃ©gitimitÃ©/Performance)
  - [x] `assign_code()` - Domain-specific code assignment
  - [x] `format_csv()` - Export results to CSV
- [x] **Remove old utilities**: citation handling, URL resolution
- [x] **Tested functionality**: All utilities working correctly with French text

### 1.6 Configuration (`backend/src/agent/configuration.py`) âœ…
- [x] **Update config options**:
  - [x] Remove: `number_of_initial_queries`, `max_research_loops`
  - [x] Add: `max_segments`, `analysis_temperature`, `segmentation_temperature`
  - [x] Update: Use Gemini 2.0 Flash as default model
- [x] **Cost optimization**: Configured for free tier usage with rate limiting

### 1.7 Token Usage Tracking & Cost Monitoring âœ… COMPLETED
- [x] **Token tracking system**:
  - [x] `TokenUsageTracker` callback handler
  - [x] Real-time token counting (input/output)
  - [x] Cost estimation for paid tier
  - [x] Free tier usage monitoring
- [x] **Performance metrics**:
  - [x] ~2,260 tokens per segment analyzed (scale validated)
  - [x] 5 LLM calls per segment (multi-step pipeline)
  - [x] $0.000254 cost per segment (Tier 1 validated)
  - [x] 60 segments/minute with Tier 1 limits

### 1.8 Scale Testing & Production Validation âœ… COMPLETED
- [x] **Scale testing with 100 segments**:
  - [x] 100% success rate (100/100 segments)
  - [x] Zero rate limit issues with Tier 1
  - [x] 6.9 minutes total processing time
  - [x] $0.0254 total cost for 100 segments
- [x] **Production readiness validation**:
  - [x] Gemini 2.0 Flash with Tier 1 billing
  - [x] 2000 RPM rate limits working perfectly
  - [x] Batch processing with error recovery
  - [x] Progress saving and resumption
- [x] **Full dataset projections**:
  - [x] 302 segments = $0.08 total cost
  - [x] ~20 minutes processing time
  - [x] <$1/year operational cost

## Phase 2: Frontend Refactoring ðŸš§ READY TO START

### 2.1 Main App (`frontend/src/App.tsx`)
- [ ] **Replace Q&A interface** with transcript analysis dashboard
- [ ] **Update data flow** for structured results display
- [ ] **Remove chat-related state** management
- [ ] **Add token usage display** for cost monitoring

### 2.2 Input Interface (`frontend/src/components/InputForm.tsx`)
- [ ] **Replace text input** with:
  - [ ] File upload for transcript files (.json from pipeline)
  - [ ] Large text area for direct paste
  - [ ] Metadata fields (group code, period if known)
  - [ ] Segment limit selector (for free tier management)
- [ ] **Update submission logic** for transcript data

### 2.3 Results Display (New Components)
- [ ] **Create `ResultsTable` component**:
  - [ ] Display 9-column table of tensions (core fields)
  - [ ] Editable cells for corrections
  - [ ] Save changes functionality
  - [ ] Row selection and highlighting
  - [ ] Export to CSV functionality
- [ ] **Create `TokenUsageDisplay` component**:
  - [ ] Real-time token usage tracking
  - [ ] Cost estimation display
  - [ ] Free tier usage warnings
- [ ] **Create `ComparisonView` component** (optional):
  - [ ] Pipeline predictions vs LLM analysis
  - [ ] Confidence scores and validation

### 2.4 Activity Timeline (Optional)
- [ ] **Repurpose for analysis progress**:
  - [ ] Show segmentation progress
  - [ ] Display analysis steps (5 steps per segment)
  - [ ] Token usage per step
  - [ ] Final summary statistics

## Phase 3: Dependencies and Infrastructure

### 3.1 NLP Dependencies
- [ ] **Add spaCy with French model**:
  - [ ] Update requirements.txt: `spacy`, `fr-core-news-lg`
  - [ ] Update Dockerfile: `RUN python -m spacy download fr_core_news_lg`
- [ ] **Optional: Additional models** (OpenAI, local French models)

### 3.2 Domain Data
- [ ] **Create taxonomy files**:
  - [ ] `backend/src/agent/taxonomy.json` - Concept mappings
  - [ ] `backend/src/agent/taxonomy.py` - Lookup functions
- [ ] **Example data**:
  - [ ] Sample transcript for testing
  - [ ] Expected output CSV

### 3.3 Docker Configuration
- [ ] **Update Dockerfile**:
  - [ ] Add spaCy installation
  - [ ] Remove Google Search dependencies
  - [ ] Update environment variables
- [ ] **Update docker-compose.yml**:
  - [ ] Maintain Postgres and Redis services
  - [ ] Update volume mounts if needed

## Phase 4: Testing and Validation

### 4.1 Unit Tests
- [ ] **Test individual nodes**:
  - [ ] Segmentation logic
  - [ ] Tension extraction
  - [ ] Classification accuracy
- [ ] **Test utility functions**:
  - [ ] Period detection
  - [ ] Theme classification
  - [ ] Code assignment

### 4.2 Integration Tests
- [ ] **End-to-end workflow** with sample transcript
- [ ] **Frontend-backend integration**
- [ ] **Docker deployment testing**

### 4.3 Expert Validation
- [ ] **Compare outputs** with expert annotations
- [ ] **Iterative prompt refinement**
- [ ] **Feedback incorporation mechanism**

## Phase 5: Documentation and Deployment

### 5.1 Documentation
- [ ] **Update README.md**:
  - [ ] New usage instructions
  - [ ] Configuration options
  - [ ] Model selection guide
- [ ] **API documentation** for new endpoints
- [ ] **User guide** for researchers

### 5.2 Production Readiness
- [ ] **Performance optimization**:
  - [ ] Concurrent processing limits
  - [ ] Memory usage monitoring
- [ ] **Error handling** and logging
- [ ] **Data persistence** and backup

## Key Technical Decisions to Discuss

1. **LLM Provider**: Continue with Gemini or switch to OpenAI/local model?
2. **Segmentation Strategy**: Rule-based (spaCy) vs LLM-based vs hybrid?
3. **Processing Approach**: Sequential vs parallel segment analysis?
4. **Validation Loop**: Include iterative refinement or single-pass?
5. **Frontend Complexity**: Simple table vs rich interactive interface?
6. **Data Storage**: LangGraph persistence vs custom database?

## ðŸ“Š Performance Metrics (Tested)
- **Token efficiency**: ~340 tokens per segment
- **API calls**: 5 calls per segment (multi-step pipeline)
- **Cost per segment**: ~$0.0001 (if paid tier)
- **Free tier capacity**: 300 segments/day
- **Processing speed**: ~30 seconds per segment
- **Accuracy**: Successfully identifies tensions matching pipeline predictions

## Estimated Timeline âœ… UPDATED
- **Phase 1 (Backend)**: âœ… COMPLETED (3 days)
- **Phase 2 (Frontend)**: 1-2 weeks
- **Phase 3 (Infrastructure)**: 1 week
- **Phase 4 (Testing)**: 1-2 weeks
- **Phase 5 (Documentation)**: 1 week

**Total**: 4-6 weeks (reduced due to successful backend completion)
